{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch Geometric Temporal\n",
    "\n",
    "Time series prediction using graphs leverages spatial information (topology) with temporal (time-based) data, leading to better forecasting in many domains, from traffic flow in urban cities to protein interactions over time.\n",
    "This notebook delves into graph time series prediction using the PyTorch Geometric Temporal library.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Loading the Dataset](#Loading-the-Dataset)\n",
    "2. [Defining the Model](#Defining-the-Model)\n",
    "3. [Setting up the Training Loop](#Setting-up-the-Training-Loop)\n",
    "4. [Evaluating Forecasting Results](#Evaluating-Forecasting-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to install PyTorch Geometric Temporal\n",
    "# !pip install torch torch-geometric torch-geometric-temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset <a name=\"Loading-the-Dataset\"></a>\n",
    "\n",
    "PyTorch Geometric Temporal provides spatiotemporal datasets tailored for graph time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "\n",
    "dataset = loader.get_dataset()\n",
    "\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model <a name=\"Defining-the-Model\"></a>\n",
    "\n",
    "We'll use a recurrent graph convolution layer followed by an LSTM for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "\n",
    "class SpatioTemporalPredictor(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(SpatioTemporalPredictor, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, 32, 1)\n",
    "        self.linear = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        return self.linear(h)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SpatioTemporalPredictor(node_features = 4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> m1 chip's mps is not supported by pytorch geometric temporal yet due to missing support for int64 from PyTorch, therefore it is necessary to use cpu for this notebook when running on mac."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Training Loop <a name=\"Setting-up-the-Training-Loop\"></a>\n",
    "\n",
    "Let's train our spatiotemporal forecasting model.\n",
    "\n",
    "Due to nature of the time sequence, the training loop is effectively a sliding window over the time series.\n",
    "And here we are accumulating the loss over the entire time series before updating the model parameters.\n",
    "There are other researchers prefer updating the model parameters at each time step, which is also possible with PyTorch Geometric Temporal.\n",
    "It is your job to decide which approach is better for your particular problem, as there is no one-size-fits-all solution here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.9836664199829102\n",
      "Epoch: 10, Loss: 0.9691984057426453\n",
      "Epoch: 20, Loss: 0.9671841263771057\n",
      "Epoch: 30, Loss: 0.9652281999588013\n",
      "Epoch: 40, Loss: 0.9639359712600708\n",
      "Epoch: 50, Loss: 0.9630054831504822\n",
      "Epoch: 60, Loss: 0.9623113870620728\n",
      "Epoch: 70, Loss: 0.9616014361381531\n",
      "Epoch: 80, Loss: 0.9610066413879395\n",
      "Epoch: 90, Loss: 0.9604488611221313\n",
      "Epoch: 100, Loss: 0.9599671363830566\n",
      "Epoch: 110, Loss: 0.9594319462776184\n",
      "Epoch: 120, Loss: 0.9589319825172424\n",
      "Epoch: 130, Loss: 0.9583826661109924\n",
      "Epoch: 140, Loss: 0.9579342603683472\n",
      "Epoch: 150, Loss: 0.957495391368866\n",
      "Epoch: 160, Loss: 0.9571177959442139\n",
      "Epoch: 170, Loss: 0.9567893743515015\n",
      "Epoch: 180, Loss: 0.9564067721366882\n",
      "Epoch: 190, Loss: 0.9559683799743652\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(200):\n",
    "    cost = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        snapshot = snapshot.to(device)\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "    cost = cost / (time+1)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Print loss for every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {cost.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Forecasting Results <a name=\"Evaluating-Forecasting-Results\"></a>\n",
    "\n",
    "It's time to evaluate the model's forecasting capability on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0564\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(test_dataset):\n",
    "    y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "    cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "cost = cost / (time+1)\n",
    "cost = cost.item()\n",
    "print(\"MSE: {:.4f}\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better prediction, one needs to increase the model capacity, train for longer, and use a more sophisticated model.\n",
    "Furthermore, one can also use Optuna to tune the hyperparameters of the model can lead to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
